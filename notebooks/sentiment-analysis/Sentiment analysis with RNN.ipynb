{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774a5634",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDb dataset with RNN\n",
    "\n",
    "We will implement a multilayer recurrent neural network (RNN) with a many-to-one architecture to predict the sentiment of IMDb reviews. First, we will load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f81e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "#silence deprecation warnings\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "#load in train and test sets\n",
    "train_imdb = IMDB(split='train')\n",
    "test_imdb = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e479a35",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Before we can feed the data into an RNN model, we need to apply several preprocessing steps:\n",
    " 1. Split the training dataset into separate training and validation partitions.\n",
    " 2. Identify the unique words in the training dataset\n",
    " 3. Map each unique word to a unique integer and encode the review text into encoded integers (an index of each unique word)\n",
    " 4. Divide the dataset into mini-batches as input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f686deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "#separate out 5000 examples for validation set from training set\n",
    "torch.manual_seed(1) #for reproducibility\n",
    "train_imdb, valid_imdb = random_split(list(train_imdb), [20000, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac95299",
   "metadata": {},
   "source": [
    "The original training dataset contains 25,000 examples. 20,000 examples are randomly chosen for training, and 5,000 for validation.\n",
    "\n",
    "We will now find the unique words (tokens) in the training dataset, which can be accomplished efficiently using the `Counter` class from the `collections` package, which is part of Pythonâ€™s standard library. To split the text into words (or tokens), we use Python's regex library to first clean the text, and then we apply the `.split()` string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d615cd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens in vocabulary: 69039\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "#define tokenizer function\n",
    "def tokenizer(text):\n",
    "    #remove all the HTML markup from the text\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    #find and store emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    #remove all non-word characters from the text and convert text into lowercase characters\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    #split text into words\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "#create Counter object to track unique tokens\n",
    "token_counts = Counter()\n",
    "for label, line in train_imdb:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "#print number of unique tokens\n",
    "print('Total unique tokens in vocabulary:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941acaa2",
   "metadata": {},
   "source": [
    "Next, we are going to map each unique word to a unique integer. The `torchtext` package already provides a class, `Vocab`, which we can use to create such a mapping and encode the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f91de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "\n",
    "#sort tokens by frequency\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#create OrderedDict mapping tokens to their frequencies\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "#create vocab object by passing in OrderedDict of tokens\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "#prepend (append to start) two special tokens\n",
    "vocab.insert_token('<pad>', 0) #padding\n",
    "vocab.insert_token('<unk>', 1) #unknown\n",
    "\n",
    "#assign unknown token by default\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d630ed",
   "metadata": {},
   "source": [
    "To demonstrate how the `vocab` object works, we apply it to a sequence of words below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77e85d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127, 7, 35, 457]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[token] for token in ['here', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e70ff",
   "metadata": {},
   "source": [
    "Observe that there will be some tokens in the validation and test sets that did not appear in the training set. As per the code above, these will be assigned the default index of $1$, and will therefore be mapped to the unknown token. Below, we define two functions to transform each text and label in the dataset to the desired encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70bf679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define lambda function to tokenize and encode text\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "\n",
    "#define lambda function to encode labels as 1 or 0\n",
    "label_pipeline = lambda x: 1. if x==2 else 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1accde51",
   "metadata": {},
   "source": [
    "Now, we want to create a `DataLoader` object for the training, validation, and test sets that uses a function `collate_batch` that employs the text and label encoding functions we wrote above and simultaneously pad sequences so that all sequences in a given mini-batch have the same length. We divide all three datasets into data loaders with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1189fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#define function to pad and encode mini-batches\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    \n",
    "    #for each text and label in batch, append encoded label, encoded text, and length of encoded text\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    \n",
    "    #make label_list and lengths into tensors\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    \n",
    "    #pad text_list\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    \n",
    "    return padded_text_list, label_list, lengths\n",
    "\n",
    "#create DataLoader with batch size 32 for the training, validation, and test set\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_imdb, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_imdb, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(list(test_imdb), batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55368a3a",
   "metadata": {},
   "source": [
    "We now want to perform feature embedding to reduce the dimensionality of the word vectors. We noted above that there are over $69000$ unique tokens in the vocabulary, which would be a very large number of input features to feed into an RNN. What's more, these features would be very sparse, as they act as a one-hot encoding for each token.\n",
    "\n",
    "A more elegant approach is to map each word to a vector of a fixed size with real-valued elements. This not only helps decrease the effect of the curse of dimensionality, but it also extracts salient features since the embedding layer in a neural network has parameters that can be learned (similar to the convolutional layers in a CNN). We implement this with PyTorch using `nn.Embedding`.\n",
    "\n",
    "### Building an RNN model\n",
    "\n",
    "We will create an RNN model for sentiment analysis, starting with an embedding layer producing word embeddings of feature size 20 (embed_dim=20). Since we have very long sequences, we are going to use an LSTM layer to account for long-range effects, which will be added next. Finally, we will add a fully connected layer as a hidden layer and another fully connected layer as the output layer, the latter of which will have a sigmoid activation function to predict the probability of the input sequence having positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f2b422e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69041, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define RNN model class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        \n",
    "        #fully connected hidden layer with ReLU activation\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #fully connected output layer with sigmoid activation\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    #define forward pass\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, \n",
    "                                                lengths.cpu().numpy(), \n",
    "                                                enforce_sorted=False, \n",
    "                                                batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "#create instance of RNN class with correct parameters\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(1) #for reproducibility\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4173116",
   "metadata": {},
   "source": [
    "Below, we define a `train` function to define the training loop and an `evaluate` function to measure the model's performance for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b38b699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define train function for one epoch\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    \n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        \n",
    "        #reset gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #generate predictions on mini-batch \n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        \n",
    "        #compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        #update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        #sum accuracy and loss on mini-batch\n",
    "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "        \n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    "\n",
    "#define evaluate function for one epoch\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    \n",
    "    with torch.no_grad(): #dont compute gradients\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            \n",
    "            #generate predictions on mini-batch\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            \n",
    "            #sum accuracy and loss on mini-batch\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            \n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c325d",
   "metadata": {},
   "source": [
    "Next, we define a binary cross-entropy loss function and choose the Adam optimizer, then we train the model for $10$ epochs and display the training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc56ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.5860 val_accuracy: 0.5486\n",
      "Epoch 1 accuracy: 0.6826 val_accuracy: 0.7746\n",
      "Epoch 2 accuracy: 0.8293 val_accuracy: 0.8326\n",
      "Epoch 3 accuracy: 0.8826 val_accuracy: 0.8534\n",
      "Epoch 4 accuracy: 0.9124 val_accuracy: 0.8544\n",
      "Epoch 5 accuracy: 0.9354 val_accuracy: 0.8610\n",
      "Epoch 6 accuracy: 0.9508 val_accuracy: 0.8552\n",
      "Epoch 7 accuracy: 0.9622 val_accuracy: 0.8460\n",
      "Epoch 8 accuracy: 0.9713 val_accuracy: 0.8730\n",
      "Epoch 9 accuracy: 0.9797 val_accuracy: 0.8660\n"
     ]
    }
   ],
   "source": [
    "#define binary cross-entropy loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "#define Adam optimizer with learning rate 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#train the model for 10 epochs\n",
    "num_epochs = 10\n",
    "torch.manual_seed(1) #for reproducibility\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcd186",
   "metadata": {},
   "source": [
    "Looking at the training and validation performance above, we can see that the model is heavily overfitting to the training set and fails to significantly improve its performance on the validation set beyond the sixth epoch. We now evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddec1559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8454\n"
     ]
    }
   ],
   "source": [
    "#evaluate model on the test set\n",
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'Test accuracy: {acc_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fc1f6",
   "metadata": {},
   "source": [
    "The model achieves an accuracy of $84.54\\%$ on the test set, which is decent, but not as good as the other method we implemented using tf-idf and a logistic regression classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
