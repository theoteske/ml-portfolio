{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe92275",
   "metadata": {},
   "source": [
    "# Sentiment analysis by fine-tuning BERT on IMDb dataset\n",
    "\n",
    "We will prepare and tokenize the IMDb movie review dataset and fine-tune a distilled BERT model to perform sentiment classification.\n",
    "\n",
    "The DistilBERT model we are using is a lightweight transformer model created by distilling a pre-trained BERT base model. The original uncased BERT base model contains over 110 million parameters while DistilBERT has 40 percent fewer parameters. Also, DistilBERT runs 60 percent faster and still preserves 95 percent of BERTâ€™s performance on the GLUE language understanding benchmark.\n",
    "\n",
    "Below, we import all the packages we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c4b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000171f",
   "metadata": {},
   "source": [
    "We will set some variables here that will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c2d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify some general settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed) #for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005550a",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Now, we can load the IMDb dataset, which consists of $50000$ reviews, each of which is labeled as having positive sentiment or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c121164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load IMDb dataset\n",
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11191252",
   "metadata": {},
   "source": [
    "Next, we split the dataset into separate training, validation, and test sets. We use $70\\%$ of the reviews for training, $10\\%$ for validation, and the remaining $20\\%$ for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cf32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 70-10-20 training-validation-test split\n",
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "valid_texts = df.iloc[35000:40000]['review'].values\n",
    "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d429764",
   "metadata": {},
   "source": [
    "Now, we tokenize the texts into individual word tokens using the tokenizer implementation inherited from the pre-trained model class. To do so, we employ the `DistilBertTokenizerFast` class, which first splits text on punctuation and whitespaces, then tokenizes each word into subword units (often referred to as wordpieces). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db48e6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29daa6e4a16c46138ca3cafacd9c09bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26639b2c4cd4f75bb8d9e6ab50fa759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3450133b7e664f028973289bd8c88249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e7fe9dffcb4e539811277735bb8288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#tokenize training, validation, and test texts\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a676c4c",
   "metadata": {},
   "source": [
    "We put each of the datasets (training, validation, and test) into a custom `Dataset` class and create corresponding data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bd02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#create custom Dataset class\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels): \n",
    "        self.encodings = encodings \n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "#create Dataset object for each of training, validation, test sets\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "#create DataLoader object with batch size 16 for each of training, validation, test sets\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461eb04c",
   "metadata": {},
   "source": [
    "### Loading and fine-tuning a pre-trained BERT model\n",
    "\n",
    "Now, we will load the pre-trained DistilBERT model and fine-tune it on the dataset created above. We specify the downstream task as sequence classification by employing the `DistilBertForSequenceClassification` class. Note as well that \"uncased\" denotes that the model does not distinguish between upper-case and lower-case characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6bdc3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9990b2aee24bf69d2c5039e9921a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pre-trained DistilBert model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bec1fe",
   "metadata": {},
   "source": [
    "Having loaded the model, we define the Adam optimizer we will be using, and specify that we will fine-tune the model for $3$ epochs. We also define a function that allows us to compute classification accuracy batch by batch to work around memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa24f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Adam optimizer with learning rate 0.00005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "#set number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "#define function to compute accuracy batch by batch\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "\n",
    "    with torch.no_grad(): #dont compute gradients\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum() \n",
    "    \n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c9a1b",
   "metadata": {},
   "source": [
    "Finally, we execute the fine-tuning loop (warning: this took a long time to run on my CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b536241b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0003 | Batch0000/2188 | Loss: 0.7027\n",
      "Epoch: 0001/0003 | Batch0250/2188 | Loss: 0.1315\n",
      "Epoch: 0001/0003 | Batch0500/2188 | Loss: 0.1684\n",
      "Epoch: 0001/0003 | Batch0750/2188 | Loss: 0.3161\n",
      "Epoch: 0001/0003 | Batch1000/2188 | Loss: 0.1067\n",
      "Epoch: 0001/0003 | Batch1250/2188 | Loss: 0.5375\n",
      "Epoch: 0001/0003 | Batch1500/2188 | Loss: 0.2163\n",
      "Epoch: 0001/0003 | Batch1750/2188 | Loss: 0.1709\n",
      "Epoch: 0001/0003 | Batch2000/2188 | Loss: 0.1655\n",
      "Training accuracy: 96.34%\n",
      " Valid accuracy: 92.66%\n",
      "Time elapsed: 577.81 min\n",
      "Epoch: 0002/0003 | Batch0000/2188 | Loss: 0.0816\n",
      "Epoch: 0002/0003 | Batch0250/2188 | Loss: 0.0810\n",
      "Epoch: 0002/0003 | Batch0500/2188 | Loss: 0.0603\n",
      "Epoch: 0002/0003 | Batch0750/2188 | Loss: 0.0445\n",
      "Epoch: 0002/0003 | Batch1000/2188 | Loss: 0.1183\n",
      "Epoch: 0002/0003 | Batch1250/2188 | Loss: 0.0400\n",
      "Epoch: 0002/0003 | Batch1500/2188 | Loss: 0.3024\n",
      "Epoch: 0002/0003 | Batch1750/2188 | Loss: 0.3865\n",
      "Epoch: 0002/0003 | Batch2000/2188 | Loss: 0.0166\n",
      "Training accuracy: 98.73%\n",
      " Valid accuracy: 92.56%\n",
      "Time elapsed: 933.62 min\n",
      "Epoch: 0003/0003 | Batch0000/2188 | Loss: 0.0881\n",
      "Epoch: 0003/0003 | Batch0250/2188 | Loss: 0.0603\n",
      "Epoch: 0003/0003 | Batch0500/2188 | Loss: 0.0128\n",
      "Epoch: 0003/0003 | Batch0750/2188 | Loss: 0.0134\n",
      "Epoch: 0003/0003 | Batch1000/2188 | Loss: 0.0886\n",
      "Epoch: 0003/0003 | Batch1250/2188 | Loss: 0.0797\n",
      "Epoch: 0003/0003 | Batch1500/2188 | Loss: 0.0158\n",
      "Epoch: 0003/0003 | Batch1750/2188 | Loss: 0.0177\n",
      "Epoch: 0003/0003 | Batch2000/2188 | Loss: 0.0185\n",
      "Training accuracy: 99.27%\n",
      " Valid accuracy: 92.34%\n",
      "Time elapsed: 1228.21 min\n",
      "Total Training Time: 1228.21 min\n",
      "Test accuracy: 92.35%\n"
     ]
    }
   ],
   "source": [
    "#track training time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dl):\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        #forward pass\n",
    "        outputs = model(input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #logging\n",
    "        if not batch_idx % 250:\n",
    "            print(f'Epoch: {epoch+1:04d}/{num_epochs:04d}'\n",
    "                  f' | Batch'\n",
    "                  f'{batch_idx:04d}/'\n",
    "                  f'{len(train_dl):04d} | '\n",
    "                  f'Loss: {loss:.4f}')\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(False): \n",
    "        print(f'Training accuracy: '\n",
    "              f'{compute_accuracy(model, train_dl, device):.2f}%'\n",
    "              f'\\n Valid accuracy: '\n",
    "              f'{compute_accuracy(model, valid_dl, device):.2f}%')\n",
    "    \n",
    "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
    "\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_dl, device):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c558146",
   "metadata": {},
   "source": [
    "As we can see above, the model definitely overfits somewhat to the training data, but it still achieves a classification accuracy of $92.35\\%$ on the test set, which is significantly better accuracy on the test set than either of the other two methods (namely, logistic regression with bag-of-words, and an RNN). Notably, validation accuracy actually decreased over the three epochs, but not by much. With greater computational power, it would be interesting to train the model over more epochs to see if validation and test accuracy increase with more training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
