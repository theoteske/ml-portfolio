{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162507e3",
   "metadata": {},
   "source": [
    "# Language modeling with an RNN\n",
    "\n",
    "In the model that we will build now, the input is a text document, and our goal is to develop a model that can generate new text that is similar in style to the input document. In character-level language modeling, the input is broken down into a sequence of characters that are fed into our network one character at a time. The network will process each new character in conjunction with the memory of the previously seen characters to predict the next one. This is inspired by the paper *Generating Text with Recurrent Neural Networks* by Ilya Sutskever, James Martens, and Geoffrey E. Hinton, Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, available at <https://pdfs.semanticscholar.org/93c2/0e38c85b69fc2d2eb314b3c1217913f7db11.pdf>.\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "We downloaded the book *The Mysterious Island*, by Jules Verne (published in 1874) in plain text format from the Project Gutenberg website at <https://www.gutenberg.org/files/1268/1268-0.txt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1194534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 1112296 \n",
      "Unique characters: 80\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#read in the text file\n",
    "with open('1268-0.txt', 'r', encoding='utf-8') as fp:\n",
    "    text = fp.read()\n",
    "\n",
    "#cut out filler from beginning and end\n",
    "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_idx = text.find('*** END OF THE PROJECT GUTENBERG')\n",
    "text = text[start_idx:end_idx]\n",
    "\n",
    "#create set of unique characters\n",
    "char_set = set(text)\n",
    "\n",
    "#display length of text and number of unique characters\n",
    "print('Total length:', len(text), '\\nUnique characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128b83d",
   "metadata": {},
   "source": [
    "To convert the text into a numeric format, we will create a Python dictionary called `char2int` that maps each character to an integer. We will also make a reverse mapping to convert the results of our model back to text, which can be done most efficiently using a NumPy array and indexing the array to map indices to the unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c7721a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary to map characters to integers\n",
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "\n",
    "#create reverse mapping via indexing a numpy array\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c1e75",
   "metadata": {},
   "source": [
    "We can look at the first few integers of the encoded text and the characters that they map to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff3090cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 <-> T\n",
      "32 <-> H\n",
      "29 <-> E\n",
      "1 <->  \n",
      "37 <-> M\n",
      "48 <-> Y\n",
      "43 <-> S\n",
      "44 <-> T\n",
      "29 <-> E\n",
      "42 <-> R\n",
      "33 <-> I\n",
      "39 <-> O\n",
      "45 <-> U\n",
      "43 <-> S\n",
      "1 <->  \n",
      "33 <-> I\n",
      "43 <-> S\n",
      "36 <-> L\n",
      "25 <-> A\n",
      "38 <-> N\n",
      "28 <-> D\n"
     ]
    }
   ],
   "source": [
    "for ex in text_encoded[:21]:\n",
    "    print('{} <-> {}'.format(ex, char_array[ex]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de4f46",
   "metadata": {},
   "source": [
    "To implement the text generation task in PyTorch, we first clip the sequence length to $40$, so that the input tensor consists of $40$ tokens. For shorter sequences, the model might focus on capturing individual words correctly while largely ignoring the context. Although longer sequences usually result in more meaningful sentences, the RNN model will have problems capturing long-range dependencies. So, we choose $40$ characters as a trade-off.\n",
    "\n",
    "We will thus split the text into chunks of size $41$: the first $40$ characters form the input sequence, and the last $40$ elements form the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91372e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/9qf196w5677gp3dkw4zjznk80000gn/T/ipykernel_24227/721936671.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#define chunks of size 41\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded)-chunk_size+1)]\n",
    "\n",
    "#define custom Dataset class to transform chunks into a dataset\n",
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text_chunks): \n",
    "        self.text_chunks = text_chunks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b50c9",
   "metadata": {},
   "source": [
    "Finally, we create a `DataLoader` object from the dataset we just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "083681d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#create DataLoader with batch size 64\n",
    "batch_size = 64\n",
    "torch.manual_seed(1) #for reproducibility\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df58763",
   "metadata": {},
   "source": [
    "### Building an RNN model\n",
    "\n",
    "The data is now ready to be loaded into an RNN model, keeping in mind that the first layer of the RNN will be an embedding layer. Below, we define an RNN model that follows the embedding layer with an LSTM layer, and finally a single fully-connected layer which outputs $80$ logits, one for each character in the vocabulary. We will sample from these model predictions to generate new text later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a52c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#define RNN class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        \n",
    "        #fully-connected layer\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "    \n",
    "    #define forward pass\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell\n",
    "    \n",
    "#specify model parameters and create an RNN model\n",
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "torch.manual_seed(1) #reproducibility\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0263591a",
   "metadata": {},
   "source": [
    "Now, we define the cross-entropy loss function and Adam optimizer before training the model for $10000$ epochs. In each epoch, we will use only one batch randomly chosen from the data loader, `seq_dl`. We will also display the training loss every 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "890d58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3721\n",
      "Epoch 500 loss: 1.3314\n",
      "Epoch 1000 loss: 1.2173\n",
      "Epoch 1500 loss: 1.2364\n",
      "Epoch 2000 loss: 1.1594\n",
      "Epoch 2500 loss: 1.1852\n",
      "Epoch 3000 loss: 1.1724\n",
      "Epoch 3500 loss: 1.1770\n",
      "Epoch 4000 loss: 1.2197\n",
      "Epoch 4500 loss: 1.1854\n",
      "Epoch 5000 loss: 1.1805\n",
      "Epoch 5500 loss: 1.0848\n",
      "Epoch 6000 loss: 1.1181\n",
      "Epoch 6500 loss: 1.1583\n",
      "Epoch 7000 loss: 1.1011\n",
      "Epoch 7500 loss: 1.1376\n",
      "Epoch 8000 loss: 1.0911\n",
      "Epoch 8500 loss: 1.1532\n",
      "Epoch 9000 loss: 1.1041\n",
      "Epoch 9500 loss: 1.1578\n"
     ]
    }
   ],
   "source": [
    "#define cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#define Adam optimizer with learning rate 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "#train the model for 10000 epochs\n",
    "num_epochs = 10000\n",
    "torch.manual_seed(1) #reproducibility\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    \n",
    "    #reset gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        #generate predictions and sum loss\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "        \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #update parameters using gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #compute loss over entire epoch\n",
    "    loss = loss.item()/seq_length\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72691f4",
   "metadata": {},
   "source": [
    "We now define a function `sample` which is fed an input string, and then generates an output string autoregressively, character by character. This means that the generated sequence is itself consumed as input for generating new characters. Note that we want to randomly sample from the logits that are output by the RNN in determining the next character, because if we only choose the highest likelihood character the model will say the same thing every time. To randomly draw these samples, we can use the class `torch.distributions.categorical.Categorical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56301515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "#define function to autoregressively generate new string from input string\n",
    "def sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
    "    \n",
    "    #encode starting_str to a sequence of integers\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "    \n",
    "    #initially assign starting_str to generated_str\n",
    "    generated_str = starting_str\n",
    "    \n",
    "    #pass encoded_input to the RNN one character at a time\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
    "    \n",
    "    #pass last character of encoded_input to RNN to generate a new character\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        \n",
    "        #obtain logits output from model\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        \n",
    "        #pass logits to Categorical to generate a new sample\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        \n",
    "        #repeat until the length of the generated string reaches the desired value\n",
    "        last_char = m.sample()\n",
    "        \n",
    "        #append new sample to end of generated string\n",
    "        generated_str += str(char_array[last_char])\n",
    "    \n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1cfa7",
   "metadata": {},
   "source": [
    "We can use the `sample` function to generate some new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e310ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island would decid the\n",
      "slopes would not recove colonists, strewn was, a tropical hold, was unaccountable, and fallen had been\n",
      "thrown that she reporter, will become that it is a greatly band, re-athought, beltigated with clean from mouth being claws, but\n",
      "a glasse to have been very further side of the island.\n",
      "\n",
      "The colonists appardulated as a clearned most a more of question was from the whole of the implicia, and even had no skinal perfected upon the island was casured by them from twending them to them\n"
     ]
    }
   ],
   "source": [
    "#generate new text using sample\n",
    "torch.manual_seed(1) #reproducibility\n",
    "print(sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa938380",
   "metadata": {},
   "source": [
    "We can further tune the training parameters, such as the length of input sequences for training, and the model architecture. We can also change the `scale_factor` parameter to change the randomness of the text being generated. As the value of `scale_factor` becomes larger than $1$, the text produced is more predictable, and as the value of `scale_factor` decreases toward zero, the text produced becomes more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0689726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island was the car was on the engineer was struck a few hours.\n",
      "\n",
      "“What a tree, and on the beach,” answered the engineer, “but not with the summit of the engineer had taken present them to the corral, which even the sea because the brig was also they were already been strewn themselves, in the presence of the mountain which appeared to last the corral. There was no other traces of the lake, and it was still some day the productions were constructed an inhabitants of the cold and running and east forth t\n"
     ]
    }
   ],
   "source": [
    "#generate text with scale_factor=2\n",
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58dcecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island would decid the\n",
      "sluce Destem 28 yiedd!” crianfaced the cabin., a tuphers. Howh, yab\n",
      "undoerae, but, at fealer, he anvednts would had quarterminated,” but\n",
      "yet My undress as frayoeh; eiging insidett,, eltigees instructed lying!0 mean, Neb dered with\n",
      "high,\n",
      "\n",
      "A !q\n",
      "Was Amer, but saufl. Towirs sharple. Natuationless azod feared\n",
      "burst reason amPnd, ammossed towards question were\n",
      "fload of petuently being him.-\n",
      "\n",
      "March Tad, Top Jup!” exclaneany?”\n",
      "\n",
      "“It is Taddy your bruish;, our mountain widen feb; themse-f\n"
     ]
    }
   ],
   "source": [
    "#generate text with scale_factor=0.5\n",
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
